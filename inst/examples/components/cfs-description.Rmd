`cfs` provides access to the NOAA long-range [Climate Forecast System](https://registry.opendata.aws/noaa-cfs/).  The CFS consists of 4 ensemble members. EFI snapshots the 6hr interval series.  In this series, the first ensemble member extends for a period of up to seven months, terminating on the latest last-day-of-the-month there-in.  The other ensembles extend up to 4 months, latest last-day-of-the-month.
EFI provides access to a [selection of 25 bands](https://github.com/eco4cast/gefs4cast/blob/main/inst/extdata/cfs-selected-bands.csv) (note these do not always match bands available in GEFS forecast product).
These data are extracted from millions of individual GRIB2 files in the NOAA AWS catalog and distributes as a set of high-performance parquet databases partitioned on `reference_datetime` and `site_id` hosted by the [Open Storage Network](https://www.openstoragenetwork.org/).
## Access

Use `arrow` for quick remote access to the database. Specifying the `reference_datetime={date}` is optional but will give the best performance.

### R 

```{r}
date <- "2020-09-24"
path <- "neon4cast-drivers/noaa/cfs/6hrly/00"
bucket <- "bio230014-bucket01"
endpoint <- "https://sdsc.osn.xsede.org"

dset <- 
  glue::glue("{bucket}/{path}/reference_datetime={date}") |>
  arrow::s3_bucket(endpoint_override = endpoint, anonymous = TRUE) |>
  arrow::open_dataset()
```

Now we can use `dplyr` commands to subset the desired data without downloading the entire data product. 

```{r}
library(dplyr)

q <- dset |> 
  filter(variable == "TMP_2m",
         site_id == "BART") |>
  group_by(datetime) |>
  summarise(temp = mean(prediction)) |>
  arrange(datetime)

df <- q |> collect()
```


### Python

```{python}
import pyarrow.dataset as ds
from pyarrow import fs

date = "2020-09-24"
path = "neon4cast-drivers/noaa/cfs/6hrly/00"
bucket = "bio230014-bucket01"
endpoint = "https://sdsc.osn.xsede.org"
s3 = fs.S3FileSystem(endpoint_override = endpoint, anonymous = True)

dset = ds.dataset(
    f"{bucket}/{path}/reference_datetime={date}",
    filesystem=s3, partitioning="hive")
    
```


```{python}
import polars as pl
q = (
  pl.scan_pyarrow_dataset(dset)
  .filter(pl.col("variable") == "TMP_2m")
  .filter(pl.col("site_id") == "BART")
  .groupby("datetime")
  .agg(pl.col("prediction").mean())
  .sort("datetime")
)
df = q.collect()
```

